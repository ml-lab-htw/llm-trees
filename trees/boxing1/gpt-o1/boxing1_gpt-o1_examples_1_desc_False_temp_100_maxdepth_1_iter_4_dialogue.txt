


```python
def predict(X: dict):
    nodes = 1 * [None]
    nodes[0] = int(X["Official"] == 1)
    
    if nodes[0]:
        prediction = 1  # Lewis
    else:
        prediction = 0  # Holyfield
    return prediction, nodes
```

**Explanation:**

The decision tree uses the **"Official"** feature to make a prediction about which fighter won the round. The tree has a maximum depth of 1 and considers whether the judge is an official judge or not.

- **Feature Used:** `"Official"`
    - **Encoding:** `1 = Yes`, `0 = No`

**Decision Logic:**

1. **Node 0:** Check if the judge is an official judge.
    - `nodes[0] = int(X["Official"] == 1)`
        - This will be `1` if the condition is true (Official judge), and `0` otherwise.
    - If `nodes[0] == 1` (Official judge):
        - **Prediction:** `1` (Lewis wins the round)
    - Else:
        - **Prediction:** `0` (Holyfield wins the round)

**Usage Example:**

```python
# Example input where the judge is an official judge
X_example = {"Judge": 0, "Official": 1, "Round": 5}

prediction, nodes = predict(X_example)

print(f"Prediction: {prediction}")  # Output: Prediction: 1
print(f"Nodes: {nodes}")            # Output: Nodes: [1]
```

- In this example, since `Official` is `1`, the condition `X["Official"] == 1` is `True`, so `nodes[0] = 1`, and the prediction is `1` (Lewis).

**Feature Keys and Encodings:**

- **Judge:** Identifier for the judge.
    - `0 = E. Williams`
    - `1 = L. O'Connell`
    - `2 = S. Christodoulu`
    - `3 = HBO-Lederman`
    - `4 = Sportsticker`
    - `5 = Boxing Times`
    - `6 = Sportsline`
    - `7 = Associated Press`
    - `8 = ESPN`
    - `9 = Boxing Monthly-Leach`
- **Official:** Indicates if the judge is an official judge.
    - `1 = Yes`
    - `0 = No`
- **Round:** Round number (1 to 12)

**Target Variable:**

- **Winner of the Round:**
    - `1 = Lewis`
    - `0 = Holyfield`

**Note:** This simple model assumes that official judges favored Lewis, while unofficial judges favored Holyfield. It only uses the "Official" feature for prediction, as per the decision tree with a maximum depth of 1.