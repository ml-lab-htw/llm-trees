


I'll create a decision tree function based on the provided features and their encodings, focusing on meaningful splits and returning both the prediction and node conditions.

```python
def predict(X: dict):
    # Initialize nodes list for storing binary decisions
    nodes = 4 * [None]
    
    # Define decision nodes with meaningful thresholds
    # Judge 3 is HBO-Lederman
    nodes[0] = X["Judge"] == 3
    # Judge 7 is Associated Press
    nodes[1] = X["Judge"] == 7
    # Judge 5 is Boxing Times
    nodes[2] = X["Judge"] == 5
    # Round threshold at middle of fight
    nodes[3] = X["Round"] <= 6
    
    # Decision tree logic
    if nodes[0]:  # HBO-Lederman
        if X["Official"] == 1:
            prediction = 1  # Lewis
        else:
            prediction = 0  # Holyfield
    else:
        if nodes[1]:  # Associated Press
            if nodes[3]:  # First half of fight
                prediction = 1  # Lewis
            else:
                prediction = 0  # Holyfield
        else:
            if nodes[2]:  # Boxing Times
                prediction = 0  # Holyfield
            else:
                prediction = 1  # Lewis
    
    return prediction, nodes
```

This decision tree:
1. Uses the actual feature names and encodings provided
2. Makes meaningful splits based on judge identities and round numbers
3. Returns both the prediction (1 for Lewis, 0 for Holyfield) and the node conditions
4. Uses logical thresholds based on the feature value ranges
5. Maintains a maximum depth of 5
6. Focuses on the most influential judges and timing of the fight

The nodes list captures the truth values of each condition in the decision path, which can be used to track the decision-making process.