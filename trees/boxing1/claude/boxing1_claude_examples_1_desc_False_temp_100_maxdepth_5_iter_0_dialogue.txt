


Based on the provided feature encoding and target variable mapping, I'll create a decision tree prediction function that aligns with my previously designed tree structure but uses the exact feature names and value encodings specified:

```python
def predict(X: dict):
    nodes = 4 * [None]  # Create list for storing node decisions
    
    # Node conditions using exact feature names and encodings
    nodes[0] = X["Judge"] == 3  # Is it HBO-Lederman?
    nodes[1] = X["Round"] <= 6
    nodes[2] = X["Official"] == 0  # Is it not an official judge?
    nodes[3] = X["Round"] <= 9
    
    # Tree logic
    if nodes[0]:  # If HBO-Lederman
        if nodes[1]:  # Round <= 6
            if nodes[2]:  # Not official judge
                prediction = 1  # Lewis
            else:
                prediction = 0  # Holyfield
        else:  # Round > 6
            if nodes[3]:  # Round <= 9
                prediction = 0  # Holyfield
            else:
                prediction = 1  # Lewis
    else:  # Other judges
        if X["Round"] <= 3:
            prediction = 1  # Lewis
        elif X["Round"] <= 7:
            prediction = 0  # Holyfield
        else:
            prediction = 1  # Lewis
            
    return prediction, nodes
```

This function:
- Takes a dictionary X with the feature names as keys
- Uses the exact feature encodings provided (Judge: 0-9, Official: 1=Yes/0=No, Round: 1-12)
- Returns a tuple containing:
  - prediction (1 for Lewis, 0 for Holyfield)
  - list of boolean values for each decision node
- Implements the decision logic from the previous tree structure
- Uses meaningful thresholds based on typical boxing round patterns