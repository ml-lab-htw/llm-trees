```python
def predict(X: dict):
    nodes = 5 * [None]
    nodes[0] = X["Judge"] <= 4  # Splitting based on Judge <= 4
    nodes[1] = X["Round"] > 6   # Splitting based on Round > 6
    nodes[2] = X["Official"] == 1  # Splitting based on Official
    nodes[3] = X["Judge"] > 1   # Splitting based on Judge > 1
    nodes[4] = X["Round"] <= 3   # Splitting based on Round <= 3


    if nodes[0]:  # Judge <= 4
        if nodes[1]: # Round > 6
            if nodes[2]: # Official == 1
                if nodes[3]: # Judge > 1
                    if nodes[4]: # Round <= 3
                        prediction = 0
                    else: # Round > 3
                        prediction = 1
                else:  # Judge <= 1
                    prediction = 0
            else:  # Official == 0
                prediction = 1
        else: # Round <= 6
            prediction = 0  
    else: # Judge > 4
        if nodes[2]: # Official == 1
            prediction = 1
        else: # Official == 0
            if nodes[4]: # Round <= 3
                prediction = 0
            else: # Round > 3
                prediction = 1

    return prediction, [int(node) if node is not None else None for node in nodes]



```

**Explanation and Reasoning:**

The goal was to create a decision tree function *without training* on the provided data.  Since no training is involved, the splits are created heuristically.  The generated function uses the given features ("Judge", "Round", "Official") to create a decision tree of depth 5.


1. **Feature Selection:**  The code uses all three available features.  A real decision tree algorithm would determine feature importance through training, but here we're simulating that process by simply incorporating all features.

2. **Splitting Criteria:** The splitting conditions within the `if/else` blocks are based on simple thresholds for each feature.  For numerical features ("Round", "Judge"), I've used greater-than/less-than comparisons. For the categorical feature "Official", I've used equality checks.

3. **Depth and Nodes:** The nested `if/else` structure ensures the tree has the desired depth. The `nodes` list stores the truth values (1 or 0) of the conditions at each inner node, allowing you to trace the path taken by the decision tree for a given input.


4. **Predictions:** The `prediction` variable is assigned a value (0 or 1) within the innermost branches of the tree. These are placeholder predictions. A trained decision tree would learn these values from data; here, they're arbitrarily assigned to illustrate how the tree structure functions.

**Important:** Because this decision tree is not trained, its predictive performance on real boxing data would likely be poor.  This code fulfills the prompt's requirements to generate a Python function representing a decision tree without using training data.  A real-world application would require training a decision tree model on a dataset to learn meaningful splits and predictions.