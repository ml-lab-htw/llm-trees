```python
def predict(X: dict):
    nodes = 5 * [None]

    nodes[0] = X["Judge"] <= 4  # Splitting on Judge
    nodes[1] = X["Round"] > 6    # Splitting on Round
    nodes[2] = X["Official"] == 1 # Splitting on Official
    nodes[3] = X["Judge"] > 1    # Splitting on Judge again
    nodes[4] = X["Round"] <= 3    # Splitting on Round again


    if nodes[0]:
        if nodes[1]:
            if nodes[2]:
                if nodes[3]:
                    if nodes[4]:
                        prediction = 0
                    else:
                        prediction = 1
                else:
                    prediction = 0
            else:
                prediction = 1
        else:
            if nodes[2]:
                prediction = 0
            else:
                prediction = 1
    else:
        if nodes[1]:
            if nodes[3]:
                if nodes[4]:
                    prediction = 1
                else:
                    prediction = 0
            else:
                prediction = 1

        else:
            if nodes[4]:
                prediction = 0
            else:
                prediction = 1


    return prediction, [int(node) for node in nodes]

```

**Explanation and Justification:**

The generated decision tree is a hardcoded structure designed to fulfill the prompt's requirements:

* **Depth:**  The tree has a maximum depth of 5, achieved through nested `if/else` conditions representing the decision nodes.
* **Feature Usage:** It uses only the provided features ("Judge", "Round", and "Official").
* **No Training:**  The tree is not trained on any data; it's a purely illustrative example. The splits are arbitrarily chosen to demonstrate the desired structure.
* **Python Function Format:** The code is formatted as a Python function that takes a dictionary `X` of features as input and returns a prediction (0 or 1) and a list of truth values for the inner nodes (1 for True, 0 for False).
* **Maximum Inner Nodes:** While the prompt specifies a possible range of 5-31 inner nodes, this example uses 5 for simplicity. A more complex tree with more nodes could be created by adding further nested conditions.

**Important Notes:**

This generated tree is NOT a meaningful model for predicting boxing round outcomes.  It serves only as a demonstration of the desired code structure and format.  To build a real predictive model, you would need to train a decision tree algorithm on actual data.  The splits and decision boundaries would then be learned from the data rather than being arbitrarily defined.