```python
def predict(X: dict):
    nodes = 3 * [None]
    nodes[0] = X["Judge"] <= 4  # Split based on Judge (first most important feature assumed)
    nodes[1] = X["Round"] <= 6   # Split based on Round (second most important feature assumed)
    nodes[2] = X["Official"] == 1 # Split based on Official (third most important feature assumed)


    if nodes[0]:
        if nodes[1]:
            if nodes[2]:
                prediction = 1
            else:
                prediction = 0
        else:
            prediction = 1
    else:
      if nodes[1]:
          prediction = 0
      else:
          prediction = 1

    return prediction, nodes

```


**Explanation and Justification:**

The task requires creating a decision tree without training, based solely on provided features and a maximum depth of 3.  Therefore, we must make reasonable assumptions about feature importance and splits.

1. **Feature Selection (Assumed Importance):**
   I've assumed "Judge", "Round", and "Official" are the most important features, in that order. This is arbitrary without training data, but a reasonable starting point. In a real-world scenario, feature importance would be determined through data analysis and model training.


2. **Splitting Criteria (Arbitrary):**

   * **Judge <= 4:**  This split divides the judges roughly into two groups.  The specific value `4` is chosen arbitrarily due to the lack of training data.
   * **Round <= 6:** This divides the fight into two halves â€“ early rounds (1-6) and later rounds (7-12).  This is a common and often significant factor in boxing.
   * **Official == 1:**  This distinguishes between official and unofficial judges, a factor that could plausibly influence scoring.



3. **Leaf Node Predictions (Arbitrary):**

   The predictions at the leaf nodes (0 or 1 representing Lewis or Holyfield winning the round) are also assigned arbitrarily since there's no training data to guide these decisions.  In a real model, these predictions would be based on the majority class within each leaf node.


4. **Depth and Inner Nodes:** The generated tree has a depth of 3 and 3 inner nodes, as requested.  It could be expanded to up to 7 inner nodes if we had more information about feature importance and splitting.


**Important Note:**  Because this decision tree is created without training data, its predictive accuracy is unknown and likely low.  It's a purely illustrative example demonstrating the structure and logic of a decision tree. To build a truly effective decision tree, you must train it on a labeled dataset.
