


Based on the decision tree I created earlier and the information you've provided, I'll format the decision tree as a Python function. Here's the implementation:

```python
def predict(X: dict):
    nodes = 4 * [None]
    nodes[0] = X["Judge"] <= 3  # Lederman is judge 3 (HBO-Lederman)
    nodes[1] = X["Round"] <= 6
    nodes[2] = X["Official"] == 1
    nodes[3] = X["Round"] <= 9

    if nodes[0]:
        if nodes[1]:
            if nodes[2]:
                prediction = 1  # de la Hoya
            else:
                prediction = 0  # Trinidad
        else:
            prediction = 0  # Trinidad
    else:
        if nodes[2]:
            if nodes[3]:
                prediction = 1  # de la Hoya
            else:
                prediction = 0  # Trinidad
        else:
            prediction = 1  # de la Hoya

    return prediction, nodes
```

This function takes a dictionary `X` with the features as input and returns a tuple containing the prediction (0 for Trinidad, 1 for de la Hoya) and a list of boolean values representing the truth values of the inner nodes.

The decision tree is structured as follows:

1. The first split is on the Judge (≤ 3, which is HBO-Lederman).
2. For judges up to HBO-Lederman:
   - If it's an early round (≤ 6), official judges favor de la Hoya, others favor Trinidad.
   - For later rounds, Trinidad is favored.
3. For judges after HBO-Lederman:
   - Official judges favor de la Hoya in earlier rounds (≤ 9) and Trinidad in later rounds.
   - Unofficial sources consistently favor de la Hoya.

This implementation captures the structure of the decision tree while adhering to the feature encodings and target variable representation you provided.