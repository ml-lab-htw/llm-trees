


Here is the Python function that implements a decision tree with a maximum depth of 1, based on the features you provided:

```python
def predict(X: dict):
    nodes = 1 * [None]
    
    # Node 0: Split based on the "Judge" feature
    nodes[0] = X["Judge"] in [0, 1, 2, 10]  # Judges: G. Hamada, B. Logist, J. Roth, van de Wiele favor Trinidad (0)
    
    if nodes[0]:  # If the Judge is G. Hamada, B. Logist, J. Roth, or van de Wiele
        prediction = 0  # Trinidad wins
    else:  # If the Judge is from HBO-Lederman, Sportsticker, etc.
        prediction = 1  # de la Hoya wins

    return prediction, nodes
```

### Explanation:
- **Input**: `X` is a dictionary that contains values for the features `Judge`, `Official`, and `Round`.
- The **decision tree** splits based on the **Judge** feature:
  - If the Judge is **G. Hamada (0), B. Logist (1), J. Roth (2), or van de Wiele (10)**, the decision is that **Trinidad (0)** wins the round.
  - Otherwise, **de la Hoya (1)** is predicted to win.
- The **nodes list** contains the truth value of the condition at the root node (whether the judge is in the group that favors Trinidad).

### Example Usage:

```python
# For example, if Judge is G. Hamada (0), Official is Yes (1), and Round is 5:
X = {"Judge": 0, "Official": 1, "Round": 5}
prediction, nodes = predict(X)

print(f"Prediction: {prediction}")  # Output: Prediction: 0 (Trinidad)
print(f"Nodes: {nodes}")  # Output: Nodes: [True]
```