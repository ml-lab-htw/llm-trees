```python
def predict(X: dict):
    nodes = 5 * [None]
    nodes[0] = X["Judge"] <= 3  # Split based on Judge <= 3
    nodes[1] = X["Round"] > 6   # Split based on Round > 6
    nodes[2] = X["Official"] == 1  # Split based on Official
    nodes[3] = X["Judge"] > 1   # Split based on Judge > 1
    nodes[4] = X["Round"] > 3   # Split based on Round > 3


    if nodes[0]:
        if nodes[1]:
            if nodes[2]:
                if nodes[3]:
                    if nodes[4]:
                        prediction = 1
                    else:
                        prediction = 0
                else:
                    prediction = 1
            else:
                prediction = 0
        else:
            if nodes[2]:
                prediction = 1 
            else:
                prediction = 0
    else:
        if nodes[1]:
            if nodes[3]:
                if nodes[4]:
                    prediction = 1
                else:
                    prediction = 0
            else:
                 prediction = 0  # Changed this to 0 to balance the tree a bit more
        else:
            if nodes[4]: # Use Round for a deeper split
                prediction = 1
            else:
                prediction = 0


    return prediction, [int(n) for n in nodes]

```



This function implements a decision tree with a maximum depth of 5.  It uses the provided features (Judge, Official, Round) to make a prediction for the target variable (which boxer won the round).  The tree's structure is hardcoded (as requested â€“ no training involved), and it aims to use the features in a reasonably logical way, considering possible decision points in a boxing match judging scenario.  For example, judges' potential biases (Judge feature),  the round number (Round feature), and whether a judge is "official" (Official feature), could all conceivably influence scoring.

The code provides a path for each combination of feature values down to a depth of 5 (or less in some cases). The predictions (0 or 1) at the leaf nodes are somewhat arbitrarily assigned in the absence of real data and training.  The essential point is the structure and the demonstration of how to create a Python function based on a given decision tree form and specified features.


If you had actual training data, you would use a library like scikit-learn to train a real decision tree classifier, resulting in a much more accurate and robust model. The output would also likely be different (both in structure and predictions). This example fulfills the prompt's request for a hardcoded, untrained decision tree.