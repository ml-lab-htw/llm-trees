```python
def predict(X: dict):
    nodes = 5 * [None]
    nodes[0] = X["Judge"] <= 4 
    nodes[1] = X["Round"] <= 6
    nodes[2] = X["Official"] == 1
    nodes[3] = X["Judge"] <= 2
    nodes[4] = X["Round"] <= 3


    if nodes[0]:
        if nodes[1]:
            if nodes[2]:
                if nodes[3]:
                    if nodes[4]:
                        prediction = 0
                    else:
                        prediction = 1
                else:
                    prediction = 0
            else:
                prediction = 1
        else:
            if nodes[3]:
                prediction = 0
            else:
                prediction = 1
    else:
        if nodes[2]:
            if nodes[4]:
                prediction = 1
            else:
                prediction = 0
        else:
           if nodes[1]:
                prediction = 1
           else:
                prediction = 0

    return prediction, [int(n) for n in nodes]
```

**Explanation of the approach:**

The task is to generate a decision tree structure *without training*.  This means we are not using any data to determine the splits. The goal is to create a valid decision tree function up to a specified depth using the given features.

The generated function follows this approach:

1. **Initialize `nodes`:** A list to store the truth values (0 or 1) of the conditions at each inner node.  The length is set by the maximum depth (5 in this case).

2. **Create conditions:**  The core part is defining the conditions at each node.  Since we aren't training, the conditions are created heuristically.  I've used various combinations of features with less than, greater than, or equal to comparisons.  These conditions could be randomized for even more variation.  I have tried to distribute the features somewhat evenly across the decision tree.

3. **Nested `if-else` structure:** The decision tree logic is implemented using nested `if-else` statements that mimic the branching of a decision tree.  The conditions stored in the `nodes` list determine the path taken through the tree.

4. **Assign Predictions:** At the leaf nodes (the end of the `if-else` chains), predictions (0 or 1 in this binary classification case) are assigned.  These could also be randomized if there's no prior knowledge about the relationship between features and the target.

5. **Return Prediction and Nodes:** The function returns the final prediction reached and the `nodes` list containing the truth values of the conditions at each inner node (converted to integers).


**Important notes:**

* This generated tree is *not optimized* in any way. It is a structural example of a decision tree of the specified depth using the provided features.
*  The performance of such a randomly generated tree on real data would likely be poor.  Actual decision tree training algorithms use data to find the best splits that maximize information gain or other criteria.
* The conditions are handcrafted and could be made more complex or randomized depending on the needs.  The distribution of features is also something that could be controlled.

This approach fulfills the requirements of generating a Python function representing a decision tree of a specified depth using the given features, without any training involved.
