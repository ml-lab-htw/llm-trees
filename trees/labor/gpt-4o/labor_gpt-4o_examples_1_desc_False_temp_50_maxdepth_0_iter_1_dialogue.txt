


Below is the Python function for the decision tree classifier based on the labor negotiation features. The function takes a dictionary `X` with the exact feature keys you provided, evaluates the decision tree, and returns the prediction along with a list representing the truth values of the inner nodes.

```python
def predict(X: dict):
    # Initialize the list of nodes (truth values for inner nodes)
    nodes = 2 * [None]
    
    # Decision Tree Logic
    nodes[0] = X["wage increase first year"] <= 4.5
    if nodes[0]:
        nodes[1] = X["contribution to health plan"] == 0
        if nodes[1]:
            prediction = 0  # bad
        else:
            prediction = 1  # good
    else:
        nodes[1] = X["vacation"] == 1  # below average
        if nodes[1]:
            prediction = 0  # bad
        else:
            prediction = 1  # good

    return prediction, nodes
```

### Explanation of the Function:
1. **Input**: The input `X` is a dictionary where the keys are feature names (e.g., `"wage increase first year"`, `"vacation"`, `"contribution to health plan"`) and the values are the corresponding feature values.
   
2. **Nodes**:
   - `nodes[0]` checks if the **wage increase in the first year** is less than or equal to 4.5%. This is the first split in the decision tree.
   - `nodes[1]` depends on the outcome of `nodes[0]`:
     - If `nodes[0]` is `True` (wage increase ≤ 4.5), `nodes[1]` checks if the **employer's contribution to the health plan** is `"none"`.
     - If `nodes[0]` is `False` (wage increase > 4.5), `nodes[1]` checks if the **number of paid vacation days** is `"ba"` (below average).

3. **Prediction**:
   - If `nodes[0]` is `True` and `nodes[1]` is `True`, the prediction is `0` (bad outcome).
   - If `nodes[0]` is `True` and `nodes[1]` is `False`, the prediction is `1` (good outcome).
   - If `nodes[0]` is `False` and `nodes[1]` is `True`, the prediction is `0` (bad outcome).
   - If `nodes[0]` is `False` and `nodes[1]` is `False`, the prediction is `1` (good outcome).

4. **Output**:
   - The function returns a tuple:
     - `prediction`: The predicted class (0 = bad, 1 = good).
     - `nodes`: A list of truth values for the inner nodes of the decision tree.

### Example Usage:
```python
# Example input
X = {
    "duration": 5,
    "wage increase first year": 4.0,
    "wage increase second year": 3.5,
    "wage increase third year": 3.0,
    "cost of living adjustment": "none",
    "working hours": 37,
    "pension": "none",
    "standby pay": 10,
    "shift differential": 5,
    "education allowance": True,
    "statutory holidays": 12,
    "vacation": "avg",
    "longterm disability assistance": True,
    "contribution to dental plan": "half",
    "bereavement assistance": True,
    "contribution to health plan": "none"
}

# Make prediction
prediction, nodes = predict(X)
print("Prediction:", prediction)  # Output: 0 (bad)
print("Nodes:", nodes)            # Output: [True, True]
```

### Example Walkthrough:
For the input `X` provided in the example:
1. `nodes[0] = X["wage increase first year"] <= 4.5` → `4.0 <= 4.5` → `True`.
2. Since `nodes[0]` is `True`, evaluate `nodes[1] = X["contribution to health plan"] == 0` → `"none" == 0` → `True`.
3. Both `nodes[0]` and `nodes[1]` are `True`, so the prediction is `0` (bad).

This structure ensures the decision tree is implemented correctly with a maximum depth of 2.
