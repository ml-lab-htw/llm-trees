


Hereâ€™s the decision tree rewritten as a Python function to classify labor negotiations as **good** (1) or **bad** (0). The function returns a prediction and a list of truth values (`nodes`) for the inner nodes:

```python
def predict(X: dict):
    nodes = 5 * [None]
    
    # Node conditions
    nodes[0] = X["duration"] <= 3  # Duration of the agreement
    nodes[1] = X["vacation"] == 1  # Below average vacation days
    nodes[2] = X["pension"] == 0  # No pension contributions
    nodes[3] = X["wage increase first year"] > 3.0  # Wage increase in first year
    nodes[4] = X["contribution to health plan"] == "full"  # Full health plan contribution

    # Decision tree logic
    if nodes[0]:  # duration <= 3
        if nodes[1]:  # vacation == 1
            if nodes[2]:  # pension == 0
                prediction = 0  # Bad outcome
            else:  # pension != 0
                prediction = 0  # Bad outcome
        else:  # vacation != 1
            prediction = 1  # Good outcome
    else:  # duration > 3
        if not nodes[3]:  # wage increase first year <= 3.0
            prediction = 0  # Bad outcome
        else:  # wage increase first year > 3.0
            if nodes[4]:  # contribution to health plan == "full"
                prediction = 1  # Good outcome
            else:  # contribution to health plan != "full"
                if X["standby pay"] <= 10:  # Standby pay <= 10
                    prediction = 0  # Bad outcome
                else:  # Standby pay > 10
                    prediction = 1  # Good outcome
    
    return prediction, nodes
```

### Explanation of the Code:
1. **Nodes and Conditions**:
   - `nodes[0]`: Checks if the **duration** of the agreement is less than or equal to 3. Shorter agreements are often associated with bad outcomes.
   - `nodes[1]`: Checks if the **vacation** days are "below average" (`"ba"`). Less vacation is associated with bad outcomes.
   - `nodes[2]`: Checks if the **pension** contribution is `"none"`. No pension contributions are indicative of a bad outcome.
   - `nodes[3]`: Checks if the **wage increase first year** is greater than 3. Higher raises indicate better outcomes.
   - `nodes[4]`: Checks if the **contribution to health plan** is `"full"`. Full health plan contributions indicate good outcomes.

2. **Decision Tree Logic**:
   - The tree follows the structure explained in the text decision tree.
   - At every level, decisions are made about whether conditions are satisfied (`True` or `False`) and the prediction proceeds along the appropriate path.
   - The output depends on the conditions satisfied. If multiple bad features (e.g., low duration, no pension, below-average vacation) are present, the outcome is **bad** (`0`). Favorable features (e.g., full health contributions, high raises, long duration) lead to a **good** outcome (`1`).

3. **Function Output**:
   - `prediction`: 0 if the negotiation outcome is classified as bad, 1 if classified as good.
   - `nodes`: A list of `1` (True) or `0` (False) corresponding to whether the condition of each inner node was satisfied.

### Example Test Case:

```python
# Test input
X = {
    "duration": 4,
    "wage increase first year": 4.0,
    "wage increase second year": 3.5,
    "wage increase third year": 3.0,
    "cost of living adjustment": "tc",
    "working hours": 37,
    "pension": "empl_contr",
    "standby pay": 12,
    "shift differential": 5,
    "education allowance": True,
    "statutory holidays": 12,
    "vacation": "avg",
    "longterm disability assistance": True,
    "contribution to dental plan": "full",
    "bereavement assistance": True,
    "contribution to health plan": "full"
}

# Run prediction
prediction, nodes = predict(X)

print("Prediction:", prediction)  # Expected output: 1 (good)
print("Nodes:", nodes)           # Expected output: [False, False, False, True, True]
```

### How It Works:
1. **Input Features**: The function takes a dictionary `X` where the keys match the feature names exactly as provided.
2. **Node Evaluations**: Conditions are evaluated in sequence, and their truth values are stored in `nodes`.
3. **Prediction**: Based on the conditions satisfied, the final class label (`0` for bad, `1` for good) is determined.

This structure satisfies the requirement for a decision tree with a depth of 5 while producing a result and tracking node evaluations.
