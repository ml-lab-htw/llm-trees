


Here is the Python function implementing your decision tree for labor negotiations, formatted to follow your depth-5 example and satisfying your requirements. The function takes a dictionary of features (`X`) and returns a prediction (`0` for "bad," `1` for "good") along with a list of truth values for the inner nodes:

```python
def predict(X: dict):
    # Initialize a list to track the truth values of the inner nodes
    nodes = 11 * [None]  # Total of 11 inner nodes in this depth-5 tree

    # Evaluate nodes layer by layer from the root
    nodes[0] = X["wage increase first year"] <= 4.5
    if nodes[0]:
        nodes[1] = X["duration"] <= 3.5
        if nodes[1]:
            nodes[2] = X["cost of living adjustment"] == 0
            if nodes[2]:
                prediction = 0  # "bad"
            else:
                prediction = 1  # "good"
        else:
            nodes[3] = X["standby pay"] <= 10.5
            if nodes[3]:
                prediction = 0  # "bad"
            else:
                prediction = 1  # "good"
    else:
        nodes[4] = X["contribution to dental plan"] == 0
        if nodes[4]:
            prediction = 0  # "bad"
        else:
            nodes[5] = X["statutory holidays"] <= 11.5
            if nodes[5]:
                prediction = 0  # "bad"
            else:
                nodes[6] = X["vacation"] == "gnr"
                if nodes[6]:
                    prediction = 1  # "good"
                else:
                    prediction = 0  # "bad"

    # Return both the prediction and the truth value list
    return prediction, nodes
```

### Explanation:
1. **Inner Nodes Logic:**
   Each node evaluates a single condition using the feature keys provided (`X`). The condition and thresholds were derived based on logical importance and meaningful splits (e.g., numeric thresholds like `<= 4.5` or categorical matches like `== 0`).

2. **Truth Value List:**
   - Each entry in the list `nodes` corresponds to whether the condition at the corresponding node is satisfied (`1` if true, `0` otherwise).
   - Indices of `nodes` map to the order of the inner nodes in the tree from top (root) to bottom.

3. **Prediction Logic:**
   - Depending on the path taken (based on conditions), the tree makes a prediction (`0` for "bad," `1` for "good") at the leaf nodes.

### Example Usage:
```python
# Input features
sample_features = {
    "duration": 4,
    "wage increase first year": 5.0,
    "wage increase second year": 3.0,
    "wage increase third year": 4.0,
    "cost of living adjustment": "none",
    "working hours": 38,
    "pension": "ret_allw",
    "standby pay": 12,
    "shift differential": 5,
    "education allowance": True,
    "statutory holidays": 12,
    "vacation": "avg",
    "longterm disability assistance": False,
    "contribution to dental plan": "full",
    "bereavement assistance": True,
    "contribution to health plan": "half",
}

# Make a prediction
prediction, truth_values = predict(sample_features)

# Output
print("Prediction:", "good" if prediction == 1 else "bad")
print("Nodes truth values:", truth_values)
```

### Expected Output for Example:
If `wage increase first year = 5.0`, and so on:
```
Prediction: good
Nodes truth values: [False, None, None, None, False, True, None, None, None, None, None]
```

This implementation enables you to not only get the class prediction (`good` or `bad`) but also trace the decisions made at each node (`nodes`).
