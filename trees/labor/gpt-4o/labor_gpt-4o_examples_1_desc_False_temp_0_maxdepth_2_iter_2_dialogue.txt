


Here is the Python function for the decision tree classifier based on the provided features and target variable. The decision tree has a maximum depth of 2, and it uses the most important features: **wage increase first year** and **contribution to health plan**.

```python
def predict(X: dict):
    """
    Predicts the outcome of labor negotiations (0 = bad, 1 = good) based on the decision tree.
    
    Parameters:
    X (dict): A dictionary containing the feature values. Keys must match the exact feature names.
    
    Returns:
    tuple: A tuple containing the prediction (0 or 1) and a list of truth values for the inner nodes.
    """
    nodes = 2 * [None]  # Initialize a list to store truth values of the inner nodes.
    
    # Node 0: Check wage increase in the first year of the contract.
    nodes[0] = X["wage increase first year"] <= 4.5
    
    if nodes[0]:  # If wage increase first year <= 4.5
        # Node 1: Check employer's contribution to health plan.
        nodes[1] = X["contribution to health plan"] == 0
        if nodes[1]:  # If contribution to health plan is "none"
            prediction = 0  # Class: bad
        else:  # If contribution to health plan is "half" or "full"
            prediction = 1  # Class: good
    else:  # If wage increase first year > 4.5
        # Node 1: Check employer's contribution to health plan.
        nodes[1] = X["contribution to health plan"] == 0
        if nodes[1]:  # If contribution to health plan is "none"
            prediction = 0  # Class: bad
        else:  # If contribution to health plan is "half" or "full"
            prediction = 1  # Class: good
    
    return prediction, nodes
```

---

### Explanation of the Function:
1. **Input**: The function takes a dictionary `X` as input, where the keys are the exact feature names (e.g., `"wage increase first year"`, `"contribution to health plan"`) and the values are the corresponding feature values.
2. **Nodes**: The `nodes` list stores the truth values of the inner nodes. Each entry in the list corresponds to whether the condition of a specific node is satisfied (`True` or `False`).
3. **Decision Tree Logic**:
   - The first split is based on `"wage increase first year"`. If it is less than or equal to 4.5, we proceed to the second split.
   - The second split is based on `"contribution to health plan"`. If it is `"none"`, the outcome is classified as `0` (bad). Otherwise, it is classified as `1` (good).
4. **Output**: The function returns a tuple containing:
   - The prediction (`0` for bad, `1` for good).
   - The list of truth values for the inner nodes.

---

### Example Usage:
```python
# Example input
X = {
    "duration": 3,
    "wage increase first year": 4.0,
    "wage increase second year": 3.5,
    "wage increase third year": 3.0,
    "cost of living adjustment": "none",
    "working hours": 37,
    "pension": "none",
    "standby pay": 10,
    "shift differential": 5,
    "education allowance": True,
    "statutory holidays": 12,
    "vacation": "avg",
    "longterm disability assistance": True,
    "contribution to dental plan": "half",
    "bereavement assistance": True,
    "contribution to health plan": "none"
}

# Make a prediction
prediction, nodes = predict(X)

# Output
print("Prediction:", prediction)  # Output: 0 (bad)
print("Nodes:", nodes)  # Output: [True, True]
```

---

### Example Walkthrough:
1. **Input**: `"wage increase first year" = 4.0` and `"contribution to health plan" = 0`.
2. **Node 0**: `4.0 <= 4.5` → `True`.
3. **Node 1**: `"none" == 0` → `True`.
4. **Prediction**: Since both conditions are satisfied, the outcome is classified as `0` (bad).

This function is flexible and can handle any valid input dictionary with the specified feature keys.
