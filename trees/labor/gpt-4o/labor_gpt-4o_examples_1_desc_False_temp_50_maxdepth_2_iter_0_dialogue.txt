


Here is the Python function for the decision tree classifier based on the features and target variable provided. The tree has a maximum depth of 2, and it uses meaningful thresholds for the splits:

```python
def predict(X: dict):
    """
    Predict whether the outcome of labor negotiations is good (1) or bad (0) based on input features.

    Parameters:
        X (dict): A dictionary containing the feature values. Keys must match the exact feature names.

    Returns:
        prediction (int): The predicted class (0 = bad, 1 = good).
        nodes (list): A list of truth values (1 if condition is satisfied, 0 otherwise) for the inner nodes of the tree.
    """
    nodes = 2 * [None]
    
    # Node 0: Check wage increase in the first year of the contract
    nodes[0] = X["wage increase first year"] <= 3.5
    
    if nodes[0]:  # wage increase first year <= 3.5
        # Node 1: Check employer's contribution towards the health plan
        nodes[1] = X["contribution to health plan"] == 0
        
        if nodes[1]:  # contribution to health plan = none
            prediction = 0  # bad
        else:  # contribution to health plan != none
            prediction = 1  # good
    else:  # wage increase first year > 3.5
        # Node 1: Check wage increase in the first year again
        nodes[1] = X["wage increase first year"] <= 5.5
        
        if nodes[1]:  # wage increase first year <= 5.5
            prediction = 1  # good
        else:  # wage increase first year > 5.5
            prediction = 0  # bad

    return prediction, nodes
```

---

### Explanation of the Function

1. **Input**:
   - The function accepts a dictionary `X` where the keys are the exact feature names, and the values are the corresponding feature values for a single instance.

2. **Nodes**:
   - `nodes` is a list that stores the truth values of the inner nodes of the decision tree. Each element in the list corresponds to whether a condition at a particular node is satisfied (`1` for True, `0` for False).

3. **Tree Logic**:
   - **Node 0**: The first split checks if the **wage increase in the first year of the contract** is less than or equal to 3.5.
   - **Node 1** (left branch): If the wage increase is less than or equal to 3.5, the next split checks if the **employer's contribution to the health plan** is `"none"`. If it is `"none"`, the prediction is `0` (bad); otherwise, it is `1` (good).
   - **Node 1** (right branch): If the wage increase is greater than 3.5, the next split checks if the **wage increase in the first year of the contract** is less than or equal to 5.5. If it is, the prediction is `1` (good); otherwise, it is `0` (bad).

4. **Output**:
   - The function returns two outputs:
     - `prediction`: The predicted class (`0` for bad, `1` for good).
     - `nodes`: A list of truth values for the inner nodes of the tree.

---

### Example Usage

```python
# Example input
X_example = {
    "duration": 5,
    "wage increase first year": 3.0,
    "wage increase second year": 4.0,
    "wage increase third year": 4.5,
    "cost of living adjustment": "none",
    "working hours": 37,
    "pension": "none",
    "standby pay": 10,
    "shift differential": 5,
    "education allowance": True,
    "statutory holidays": 12,
    "vacation": "avg",
    "longterm disability assistance": True,
    "contribution to dental plan": "half",
    "bereavement assistance": True,
    "contribution to health plan": "none"
}

# Predict
prediction, nodes = predict(X_example)

# Output
print("Prediction:", prediction)  # Output: 0 (bad)
print("Nodes:", nodes)  # Output: [1, 1]
```

---

### Example Walkthrough

For the example input:
- **Node 0**: `wage increase first year = 3.0`. Since `3.0 <= 3.5`, `nodes[0] = 1`.
- **Node 1**: `contribution to health plan = 0`. Since it is `"none"`, `nodes[1] = 1`.
- Prediction: Since both conditions are satisfied, the prediction is `0` (bad).

This function can now be used to classify outcomes and understand the decision-making process of the tree.
