


Here is the Python function for the decision tree classifier based on the features and target variable you provided. The decision tree has a maximum depth of 2, and it uses meaningful thresholds for the splits:

```python
def predict(X: dict):
    """
    Predicts the outcome of labor negotiations (0 = bad, 1 = good) based on input features.
    
    Parameters:
        X (dict): A dictionary containing the feature values. Keys must match the feature names exactly.
    
    Returns:
        prediction (int): The predicted class (0 = bad, 1 = good).
        nodes (list): A list of truth values (1 if the condition is satisfied, 0 otherwise) for the inner nodes.
    """
    nodes = 2 * [None]
    
    # Node 0: Split on "wage increase first year"
    nodes[0] = X["wage increase first year"] <= 4.5
    
    if nodes[0]:  # If "wage increase first year" <= 4.5
        # Node 1: Split on "contribution to health plan"
        nodes[1] = X["contribution to health plan"] == 0
        if nodes[1]:  # If "contribution to health plan" == 0
            prediction = 0  # bad
        else:  # If "contribution to health plan" != 0
            prediction = 1  # good
    else:  # If "wage increase first year" > 4.5
        # Node 1: Split on "vacation"
        nodes[1] = X["vacation"] == 1  # "ba" = below average
        if nodes[1]:  # If "vacation" == 1
            prediction = 0  # bad
        else:  # If "vacation" != 1
            prediction = 1  # good
    
    return prediction, nodes
```

### Explanation of the Function:
1. **Input:**
   - The function takes a dictionary `X` as input, where the keys are the feature names (e.g., `"wage increase first year"`, `"contribution to health plan"`, etc.), and the values are the corresponding feature values.

2. **Nodes:**
   - `nodes` is a list that stores the truth values of the inner nodes in the decision tree. Each entry is `1` if the condition of the corresponding node is satisfied, and `0` otherwise.

3. **Decision Tree Logic:**
   - **Node 0:** The first split is based on `"wage increase first year"`. If it is less than or equal to 4.5, the tree moves to the left; otherwise, it moves to the right.
   - **Node 1 (Left Subtree):** If `"wage increase first year" <= 4.5`, the next split is based on `"contribution to health plan"`. If it is `"none"`, the outcome is classified as `0` (bad); otherwise, it is classified as `1` (good).
   - **Node 1 (Right Subtree):** If `"wage increase first year" > 4.5`, the next split is based on `"vacation"`. If it is `"ba"` (below average), the outcome is classified as `0` (bad); otherwise, it is classified as `1` (good).

4. **Output:**
   - The function returns:
     - `prediction`: The predicted class (0 = bad, 1 = good).
     - `nodes`: A list of truth values for the inner nodes.

### Example Usage:
```python
# Example input
X = {
    "duration": 3,
    "wage increase first year": 4.0,
    "wage increase second year": 3.5,
    "wage increase third year": 3.0,
    "cost of living adjustment": "none",
    "working hours": 37,
    "pension": "ret_allw",
    "standby pay": 10,
    "shift differential": 5,
    "education allowance": True,
    "statutory holidays": 12,
    "vacation": "avg",
    "longterm disability assistance": True,
    "contribution to dental plan": "half",
    "bereavement assistance": True,
    "contribution to health plan": "none"
}

# Predict
prediction, nodes = predict(X)
print("Prediction:", prediction)  # Output: 0 (bad)
print("Nodes:", nodes)  # Output: [1, 1]
```

### Example Output:
For the input above:
- `"wage increase first year" = 4.0` satisfies the condition `<= 4.5` (Node 0 → `1`).
- `"contribution to health plan" = 0` satisfies the condition `== 0` (Node 1 → `1`).
- The prediction is `0` (bad).
