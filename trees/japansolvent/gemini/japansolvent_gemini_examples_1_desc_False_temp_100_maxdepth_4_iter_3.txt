```python
def predict(X: dict):
    nodes = 15 * [None]  # Initialize a list for all possible nodes (up to 15)

    nodes[0] = X["EBIT/TA"] > 0.05
    nodes[1] = X["NI/TC"] > 0.02
    nodes[2] = X["Sales/TA"] > 1.5
    nodes[3] = X["EBIT/Sales"] > 0.1
    nodes[4] = X["NI/Sales"] > 0.05
    nodes[5] = X["WC/TA"] > 0.2
    nodes[6] = X["Equity/TL"] > 0.5
    nodes[7] = X["Equity/TA"] > 0.4



    if nodes[0]:
        if nodes[1]:
            if nodes[2]:
                if nodes[3]:
                    prediction = 1
                else:
                    prediction = 0
            else:
                 if nodes[4]:
                    prediction = 1
                 else:
                    prediction = 0
        else:
             if nodes[5]:
                if nodes[6]:
                    prediction = 1
                else:
                    prediction = 0
             else:
                prediction = 0

    else: # EBIT/TA <= 0.05
        if nodes[6]: # Equity/TL > 0.5
            if nodes[7]: # Equity/TA > 0.4
                 if nodes[5]:
                    prediction = 1
                 else:
                    prediction = 0
            else:  # Equity/TA <= 0.4
                 prediction = 0

        else: # Equity/TL <= 0.5
            prediction = 0



    return prediction, nodes[:8] # Return only the used nodes (up to depth 4)


```


This code creates a decision tree function with a maximum depth of 4. It uses a nested `if-else` structure corresponding to the tree's branches. The conditions at each node are based on threshold values for different features (these thresholds are chosen arbitrarily as we're not training on real data). The `nodes` list stores the truth values (1 or 0) of the conditions at each inner node, enabling traceability of the decision path.  The function returns both the final prediction (0 or 1) and the `nodes` list.



Important: This generated tree is entirely symbolic and not optimized for predictive performance.  To create a truly effective decision tree, you'd need to train it on a labeled dataset using algorithms like ID3, C4.5, or CART, which determine the best features and split points based on the data.  This example demonstrates the structure of a decision tree function but does not perform actual learning.