```python
def predict(X: dict):
    nodes = 10 * [None]  # Initialize a list to store truth values of inner nodes

    nodes[0] = X["EBIT/TA"] > 0.05
    nodes[1] = X["NI/TC"] > 0.02
    nodes[2] = X["Sales/TA"] > 1.2
    nodes[3] = X["EBIT/Sales"] > 0.1
    nodes[4] = X["NI/Sales"] > 0.05
    nodes[5] = X["WC/TA"] > 0.2
    nodes[6] = X["Equity/TL"] > 0.8
    nodes[7] = X["Equity/TA"] > 0.5
    nodes[8] = X["Sales/TA"] > 1.5  # Reusing a feature for demonstration
    nodes[9] = X["EBIT/TA"] > 0.1


    if nodes[0]:
        if nodes[1]:
            if nodes[2]:
                if nodes[3]:
                    prediction = 1
                else:
                    prediction = 0
            else:
                if nodes[4]:
                    prediction = 1
                else:
                    prediction = 0
        else:
            if nodes[5]:
                if nodes[6]:
                    prediction = 1
                else:
                    prediction = 0
            else:
                prediction = 0
    else:
        if nodes[7]:
            if nodes[8]:
                if nodes[9]:
                    prediction = 1
                else:
                    prediction = 0
            else:
                prediction = 0
        else:
            prediction = 0


    return prediction, nodes
```


This decision tree uses the provided features in a nested `if-else` structure, creating a tree-like decision process.  The `nodes` list stores the truth value (1 or 0) of each condition, allowing you to track the path taken through the tree for a given input.  The tree has a maximum depth of 4 (as requested), and uses between 4 and 15 inner nodes (in this example, it uses 10).  It's important to remember that this tree is **not trained** on any data and is just a demonstration of the structure and format requested.  The thresholds used in the conditions are arbitrary examples.  A real decision tree would learn these thresholds from training data to maximize predictive accuracy.