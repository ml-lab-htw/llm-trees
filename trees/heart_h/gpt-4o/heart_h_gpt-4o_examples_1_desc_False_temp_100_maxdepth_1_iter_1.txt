Here's a decision tree with a maximum depth of 1 based on the Features you provided. Since the tree can only have one decision node, I will choose a feature that is commonly considered a strong predictor of heart disease: "ca" (Number of Major Vessels Colored by Fluoroscopy).

The decision rule will be: if the number of major vessels (ca) is 0, the prediction will be no heart disease (0). Otherwise, the prediction will be heart disease (1). This is a simple decision based on the importance of vessel blockages in predicting heart disease.

```python
def predict(X: dict):
    nodes = 1 * [None]
    nodes[0] = X["ca"] == 0

    if nodes[0]:
        prediction = 0
    else:
        prediction = 1

    return prediction, nodes
```

### Explanation:
- The decision tree has a single inner node based on the feature "ca".
- If the number of major vessels colored by fluoroscopy (`ca`) is 0, the prediction is no heart disease (`0`).
- Otherwise, the prediction is heart disease (`1`).
- The `nodes` list holds the truth value of the condition, which can be useful for debugging or understanding the decision path.

#### Example usage:
```python
# Example input
X = {
    "age": 45,
    "sex": 1,  # Male
    "chest_pain": 0,
    "trestbps": 130,
    "chol": 250,
    "fbs": 2,
    "restecg": 2,
    "thalach": 150,
    "exang": 1,
    "oldpeak": 1.0,
    "slope": 2,
    "ca": 1,  # 1 major vessel colored
    "thal": 2
}

# Prediction
prediction, nodes = predict(X)
print(f"Prediction: {prediction}, Nodes: {nodes}")
```

In this example, because `"ca": 1`, the decision node is not satisfied, and the prediction is `1` (heart disease). The `nodes` list will contain `[0]`, indicating the condition was not satisfied.
