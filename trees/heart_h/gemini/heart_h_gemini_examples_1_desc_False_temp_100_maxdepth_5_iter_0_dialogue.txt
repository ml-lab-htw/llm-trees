


```python
def predict(X: dict):
    nodes = 7 * [None]

    nodes[0] = X["chest_pain"] == 0  # Asymptomatic
    nodes[1] = X["oldpeak"] > 2.0
    nodes[2] = X["ca"] > 0
    nodes[3] = X["exang"] == 2  # Yes
    nodes[4] = X["slope"] == 1  # Downsloping
    nodes[5] = X["age"] > 60
    nodes[6] = X["trestbps"] > 140


    if nodes[0]:
        if nodes[1]:
            prediction = 1  # Heart Disease
        else:
            if nodes[2]:
                prediction = 1  # Heart Disease
            else:
                prediction = 0  # No Heart Disease
    else:
        if nodes[3]:  # Exercise Induced Angina
            if nodes[4]: # Downsloping
                prediction = 1 # Heart Disease
            else:
                if nodes[5]: # Age > 60
                    prediction = 1 # Heart Disease
                else:
                    prediction = 0 # No Heart Disease

        else:  # No Exercise Induced Angina
            if X["thal"] == 3: # Reversible Defect
                prediction = 0 # No Heart Disease
            else:
                if nodes[6]:  # Resting BP > 140
                    prediction = 1  # Heart Disease
                else:
                    prediction = 0  # No Heart Disease


    return prediction, nodes[:sum(n is not None for n in nodes)]  # Return only used nodes

```


This function mirrors the decision tree logic, using the provided feature names and encodings.  The `nodes` list tracks the truth values of the conditions at each node.  The function returns the prediction (0 or 1) and the list of node truth values.  The last line ensures that only the nodes that are actually used in the tree are returned, avoiding unnecessary None values in the list.  This addresses potential issues with variable-length `nodes` lists depending on the input features.
