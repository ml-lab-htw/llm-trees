```python
def predict(X: dict):
    nodes = 3 * [None]

    nodes[0] = X["Age"] <= 30  # Example split, adjust as needed
    nodes[1] = X["BMI_FP"] > 25 # Example split, adjust as needed
    nodes[2] = X["Lysholm"] <= 70 # Example split, adjust as needed

    if nodes[0]:
        prediction = 0  # Example prediction, adjust as needed
    else:
        if nodes[1]:
            if nodes[2]:
                prediction = 1
            else:
                prediction = 0
        else:
            prediction = 1


    return prediction, nodes

```

**Explanation and Considerations:**

1. **Feature Selection:**  I've chosen `Age`, `BMI_FP`, and `Lysholm` as examples for the decision tree splits.  These were selected based on potential relevance to cartilage health and recovery after ACL injury.  In a real application, feature importance would be determined through analysis of data (e.g., correlation analysis, feature importance from other models). You should carefully consider domain knowledge or insights from exploratory data analysis to select the most relevant features.

2. **Split Points:** The chosen split points (e.g., `Age <= 30`, `BMI_FP > 25`, `Lysholm <= 70`) are also examples. In a real-world scenario, these values should be determined based on data distributions and optimal split points found during model training.

3. **Predictions:** The predictions (0 or 1) at the leaf nodes are also placeholders.  These would be determined by the majority class in the training data that falls into that leaf node.

4. **Depth and Nodes:** The code ensures the tree has a maximum depth of 3, meaning the longest path from the root to a leaf is 3 edges.  The number of inner nodes is variable but will be between 3 and 7 in a full binary tree of depth 3 (but can be less if any branches end early).

5. **No Training Data:** As requested, this code generates a decision tree structure *without* training on data.  The resulting tree is therefore not optimized for performance and serves as a template for how to structure such a function.

**To make this a truly useful decision tree, you would need to:**

* **Train it on a labeled dataset:** Use a decision tree learning algorithm (like those in scikit-learn) with your data to determine the best features, split points, and predictions at each node.
* **Evaluate performance:** Use metrics like accuracy, precision, and recall to assess how well the trained tree predicts cartilage thickness change.
* **Tune hyperparameters:** Adjust the maximum depth, minimum samples per leaf, and other parameters to optimize the tree's performance and prevent overfitting.



The provided code fulfills the prompt's requirements for generating the function structure and logic without training, but for a practical application, you *must* train it with real data.