```python
def predict(X: dict):
    nodes = 1 * [None]
    nodes[0] = X["ccmfthctabame"] <= 2.0  # Using baseline cartilage thickness as a split point

    if nodes[0]:
        prediction = 0  # Predict normal change if baseline thickness is low
    else:
        prediction = 1  # Predict abnormal change if baseline thickness is high
    return prediction, nodes

```

**Explanation of Decision Tree Construction (without data):**

Since we're building a decision tree of depth 1 without training data, we need to select a single feature and a splitting criterion.  The goal is to choose a feature and a split that might logically relate to cartilage thickness change.  

I've chosen `ccmfthctabame` (baseline mean cartilage thickness) as the feature.  The rationale is that a thinner cartilage at baseline might be more likely to show abnormal changes over time.

The split point of 2.0 mm is arbitrary in the absence of data. In a real-world scenario, this split point would be determined by an algorithm optimizing for information gain or Gini impurity based on actual training data.

**Key Points:**

* **Feature Selection (without data):**  Without training data, feature selection becomes heuristic.  I selected `ccmfthctabame` based on domain intuition (thinner cartilage might be more prone to change).  In a real application, feature importance would be determined using data.
* **Split Point (without data):** The split point of 2.0 is arbitrary.  With data, a decision tree algorithm would learn the optimal split point.
* **Depth 1:** The `if/else` structure ensures that the tree has a single decision node (depth 1).
* **Python Function Format:** The function takes a dictionary `X` of features and returns both the prediction and a list representing the truth values of the inner nodes (which in this case is just one node).



This solution demonstrates the requested format and logic for building a simple decision tree without using training data.  However, for a useful model, training on actual data is crucial.
