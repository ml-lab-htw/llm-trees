


```python
def predict(X: dict):
    """
    Predicts whether a student completes their school leaving certificate.

    Args:
        X: A dictionary of features.

    Returns:
        A tuple containing the prediction (0 or 1) and a list of truth values
        for the inner nodes.
    """
    nodes = 4 * [None]

    # Using meaningful thresholds based on the feature descriptions
    nodes[0] = X["Type_school"] == 0  # Primary terminal leaver
    nodes[1] = X["Educational_level"] <= 8 # Below Senior Cycle
    nodes[2] = X["DVRT"] <= 75  # Assuming 75 is a reasonable threshold
    nodes[3] = X["Prestige_score"] <= 15 # Assuming 15 is a meaningful split

    if nodes[0]:  # Primary terminal leaver
        prediction = 0 # Assumed they don't get SLC
    else:
        if nodes[1]: # Below Senior Cycle
            prediction = 0 # Less likely to complete SLC
        else: # Senior Cycle or higher
            if nodes[2]: # Low DVRT Score
                if nodes[3]: # Lower prestige score
                    prediction = 0
                else:  # Higher prestige score (might slightly increase chance)
                    prediction = 1
            else:  # High DVRT Score
                prediction = 1 # More likely to complete SLC

    return prediction, nodes

# Example usage:
X = {
    'Sex': 1,
    'DVRT': 90,
    'Educational_level': 10,
    'Prestige_score': 20,
    'Type_school': 1
}
prediction, nodes = predict(X)
print(f"Prediction: {prediction}")
print(f"Nodes: {nodes}")


X = {
    'Sex': 0,
    'DVRT': 60,
    'Educational_level': 7,
    'Prestige_score': 10,
    'Type_school': 0
}
prediction, nodes = predict(X)
print(f"Prediction: {prediction}")
print(f"Nodes: {nodes}")

X = {
    'Sex': 1,
    'DVRT': 80,
    'Educational_level': 9,
    'Prestige_score': 12,
    'Type_school': 2
}

prediction, nodes = predict(X)
print(f"Prediction: {prediction}")
print(f"Nodes: {nodes}")

```


**Key Improvements and Explanations:**

* **Meaningful Thresholds:** The code now uses more reasonable threshold values for `DVRT` (75) and `Prestige_score` (15).  These are still hypothetical but represent a more realistic scenario.  Ideally, these thresholds would be learned from training data using a decision tree learning algorithm.
* **Clearer Logic:**  The nested `if-else` structure directly reflects the decision tree logic, making it easier to understand.
* **Adherence to Feature Encoding:** The code strictly uses the provided feature encoding (0/1 for sex, 0-10 for educational level, 0-28 for prestige score, 0-2 for school type).
* **Example Usage:** Added examples to demonstrate how to use the `predict` function.
* **Node List:** The `nodes` list correctly reflects the truth values of the conditions at each inner node.



**Important Note:** This function represents a *hypothetical* decision tree based on the limited information available. To build a truly effective decision tree, you *must* train it on a dataset using a decision tree learning algorithm.  This will determine the optimal thresholds and feature splits to maximize predictive accuracy.  Libraries like scikit-learn in Python provide tools to do this.